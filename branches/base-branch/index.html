
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">Iso-LWGAN: A Geometry-Aware Approach to Adaptive Generative Modeling</h2>

<section>
  <h2>Abstract</h2>
  <p>We introduce Iso-LWGAN, an adaptive dimension-learning framework that builds on the Latent Wasserstein GAN (LWGAN) by integrating an isometric regularization term and partial stochasticity in the generator. Our goal is to address the challenge of learning data manifolds whose intrinsic dimensions are lower than those of the ambient space while also preserving local distances for enhanced interpretability.</p>
  <p>The isometric regularizer encourages the generator to maintain geometric fidelity between latent codes and output samples, whereas the partially stochastic generator captures the multimodality often present in real-world data. This combination tackles issues of latent mismatch and mode collapse reported in previous GAN-based methods.</p>
  <p>We quantitatively and qualitatively validate Iso-LWGAN on synthetic manifolds and real datasets such as MNIST, showing that our method preserves local geometry, detects manifold dimensions, and achieves improved coverage of data modes. Experiments reveal that aligning latent distances with generated sample distances leads to smoother interpolations, whereas introducing carefully controlled noise inside the generator helps mitigate mode collapse.</p>
  <p>Our results underscore Iso-LWGAN’s potential as a powerful and flexible tool for manifold-aware generative modeling.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Generative modeling seeks to learn underlying distributions from observed data, enabling the generation of new, high-quality samples that approximate real-world complexity. Classic latent-variable models such as Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) have demonstrated remarkable success in areas including image synthesis, text generation, and audio modeling. However, many real-world datasets lie on lower-dimensional manifolds embedded within high-dimensional ambient spaces, meaning that arbitrarily chosen latent dimensions may fail to capture important geometric structures and meaningful content variations.</p>
  <p>Recent work has emphasized the benefits of manifold-awareness for generative models. The Latent Wasserstein GAN (LWGAN) was proposed to adaptively estimate the manifold’s intrinsic dimension by learning a latent distribution with a diagonal covariance matrix whose rank corresponds to the data dimensionality. LWGAN combines concepts from Wasserstein Auto-Encoders (WAE) and Wasserstein GANs (WGAN) to yield consistent dimension estimates, but limitations persist. In particular, the generator is purely deterministic and may not accommodate complex multimodal distributions effectively, and there are no explicit guarantees that local geometric relationships will be preserved.</p>
  <p>Concurrently, isometric or distance-preserving techniques have gained traction in representation learning. These methods prioritize the principle that small perturbations in the latent codes should induce proportional, continuous changes in the generated outputs, thus improving interpretability and stability. Strictly deterministic mappings, however, can limit coverage of diverse data modes, reducing the model’s capacity to distribute probability mass over the often multimodal data manifold.</p>
  <p>In this paper, we propose Iso-LWGAN, an enhancement over LWGAN that addresses these gaps by incorporating an isometric regularizer and introducing partial stochasticity in generation. The key contributions are:</p>
  <ul>
    <li><strong>Isometric Regularization:</strong> Incorporates a distance-preserving penalty to align local distances in the latent and output spaces, yielding smoother transitions and fewer geometric distortions.</li>
    <li><strong>Noise Injection:</strong> Injects controlled noise into the generator to strike a balance between deterministic and fully stochastic mappings, helping to avoid mode collapse while maintaining training stability.</li>
    <li><strong>Latent Adaptivity:</strong> Preserves the diagonal latent-covariance parameterization from LWGAN, ensuring that the learned dimensionality matches the manifold dimension.</li>
    <li><strong>Experimental Analysis:</strong> Conducts experiments that isolate the effect of isometric regularization, analyze the benefits of partial noise, and compare Iso-LWGAN against the baseline LWGAN on MNIST.</li>
  </ul>
  <p>The remainder of this paper is organized as follows. We first position our approach in relation to prior manifold-based and geometry-conscious generative methods. Next, we summarize background concepts on dimension-adaptive latent variable models. We then detail the Iso-LWGAN architecture, focusing on how the isometric penalty and partial noise injection integrate with LWGAN’s dimension consistency. We follow with our experimental setup on synthetic data and real data (MNIST). Finally, we present comprehensive results showing local geometry improvements, better coverage, and structured latent spaces, and conclude with reflections on future directions for manifold-aware generative modeling.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>Generative adversarial networks have progressed substantially since their inception, largely focusing on broader distribution matching in Euclidean space. However, manifold-awareness has become increasingly important, leading to methods that deliberately account for the possibility that real data occupy a constrained subspace. For example, manifold-aware extensions to VAEs have replaced simple Gaussian priors with more expressive distributions to accommodate curved or otherwise non-Euclidean data supports.</p>
  <p>Dimension-consistent techniques specifically aim to match the latent space dimension to that of the underlying data manifold. LWGAN is one such approach, learning a diagonal covariance for the latent distribution so that the effective rank matches the intrinsic dimension. Meanwhile, WGAN variants rely on a Wasserstein metric for stable training, but they typically do not tackle dimension estimation or local distance preservation.</p>
  <p>Beyond dimension adaptivity, an emerging research direction explores isometric representation learning, wherein the goal is to preserve local distances. Such ideas have proven beneficial in supervised scenarios like metric learning but are only beginning to gain traction in generative modeling. Ensuring that latent-space perturbations map to proportionate changes in data space can help ameliorate discontinuities or geometric distortions in the learned manifold.</p>
  <p>Partial or selective noise injection has also been proposed to mitigate mode collapse. Although earlier approaches often used purely deterministic or purely stochastic generators, these extremes each have drawbacks in coverage versus stability. A “hybrid” methodology—feeding a modest noise vector alongside the latent code—can expand the distribution of outputs for each latent code without severely disrupting training. Iso-LWGAN consolidates these strands by combining dimension-adaptive priors, isometric constraints, and partial stochasticity.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Real-world data frequently reside on manifolds of lower dimension than the space in which they physically appear. LWGAN addresses this by letting the latent prior be a multivariate Gaussian with diagonal covariance, where some variances may become negligible, effectively reducing the latent dimension. Formally, if the data manifold X ⊂ ℝ<sup>D</sup> has dimension m, then a rank-m diagonal matrix A captures variance in m directions while suppressing variance in the others. The learned parameters dictate which directions remain active.</p>
  <p>Adversarial training in LWGAN is based on minimizing the Wasserstein distance between real and generated data distributions. One typically enforces a 1-Lipschitz constraint on the critic (discriminator) using gradient penalty or a similar technique.</p>
  <p>Although dimension adaptation offers benefits for manifold alignment, local geometry could still be distorted because the WGAN objective does not explicitly penalize differences in local distances. Moreover, a purely deterministic generator can struggle to reproduce multimodal distributions, leading to partial collapse of modes. Integrating an isometric constraint and partial noise injection can address these problems:</p>
  <ul>
    <li><strong>Isometric Representation Learning:</strong> We incorporate a penalty on the absolute difference between pairwise distances in latent codes and generated outputs, thereby preserving local geometry.</li>
    <li><strong>Partial Stochasticity:</strong> By concatenating a small noise vector ε with the latent code z, we permit multiple potential outcomes from the same z, enhancing coverage and reducing collapse. The parameter σ governs how strongly the noise influences generation, balancing the reliability of a deterministic map with the expressiveness of a stochastic one.</li>
  </ul>
</section>

<section>
  <h2>Method</h2>
  <p>Iso-LWGAN synthesizes three main ideas for manifold-aware generative modeling: dimension adaptation, isometric regularization, and partial stochasticity.</p>
  <ul>
    <li><strong>Adaptive Latent Dimensionality:</strong> Adopts the LWGAN notion of a diagonal covariance matrix A for the latent normal distribution. By learning which directions in ℝ<sup>d</sup> are truly active, the model approximates the intrinsic manifold dimension. The overall architecture includes an encoder Q: X → Z that maps data to latent codes and a generator G: Z → X that attempts to reconstruct data from latent codes.</li>
    <li><strong>Isometric Regularization:</strong> Promotes distance preservation by adding a term that penalizes the mismatch between pairwise distances. For latent codes z<sub>i</sub> and z<sub>j</sub> and generated points x<sub>i</sub> and x<sub>j</sub>, the loss is defined as L<sub>iso</sub> = λ<sub>iso</sub> * (1/N²) * Σ<sub>ij</sub> | ||z<sub>i</sub> - z<sub>j</sub>||₂ - ||x<sub>i</sub> - x<sub>j</sub>||₂ |. The hyperparameter λ<sub>iso</sub> controls how strictly local distances must match between the latent and generated spaces.</li>
    <li><strong>Partially Stochastic Generator:</strong> Instead of a fully deterministic generator, a noise vector ε drawn from a low-dimensional normal distribution N(0, σ²I) is concatenated with the latent code z to form the generator input [z; ε]. A suitably chosen σ helps expand the range of generated outputs without compromising training stability. This allows a single latent code to map to multiple data modes if the real data exhibits distinct clusters.</li>
  </ul>
  <p>The overall training objective retains the standard WGAN-based adversarial approach, possibly augmented with reconstruction and consistency terms for encoding as in LWGAN. The total loss is given by:</p>
  <pre><code>L_total = L_Wasserstein + L_iso</code></pre>
  <p>This loss balances stable manifold coverage, local geometry preservation, and dimension adaptivity in one framework.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>The contributions of the isometric regularizer and partial generator noise are verified through three separate experiments:</p>
  <p><strong>Geometry Preservation on Synthetic 2D Data</strong></p>
  <ul>
    <li><strong>Dataset:</strong> A two-component Gaussian mixture in ℝ<sup>2</sup>.</li>
    <li><strong>Goal:</strong> Observe how varying λ<sub>iso</sub> ∈ {0, 0.1, 0.5, 1.0, 2.0} influences distance alignment. Metrics include the average absolute difference between pairwise distances in the latent and generated spaces, reconstruction error, and visualization of latent interpolations.</li>
    <li><strong>Architecture:</strong> A simple encoder and generator with two hidden layers, using the Adam optimizer over a moderate number of epochs.</li>
  </ul>
  <p><strong>Partially Stochastic Generator for Multimodal Data</strong></p>
  <ul>
    <li><strong>Dataset:</strong> A 2D multimodal dataset with multiple clusters.</li>
    <li><strong>Goal:</strong> Investigate partial noise injection by varying σ_noise ∈ {0, 0.1, 0.5} to assess improvements in mode coverage and reduction in mode collapse.</li>
    <li><strong>Metrics:</strong> Qualitative inspection of sample diversity and measurements of training stability under different noise levels.</li>
  </ul>
  <p><strong>MNIST Comparison: Iso-LWGAN vs. Base LWGAN</strong></p>
  <ul>
    <li><strong>Dataset:</strong> MNIST digits (28×28 images), where the image space is 784-dimensional and the inherent manifold dimension is presumably much lower.</li>
    <li><strong>Goal:</strong> Directly compare a baseline LWGAN (without isometric regularization and noise injection) to Iso-LWGAN (with nonzero λ<sub>iso</sub> and partial noise). Both models share a feedforward encoder/generator architecture aside from the noise injection difference.</li>
    <li><strong>Measurements:</strong> Tracking reconstruction and total losses, examining digit coverage, generating latent interpolations, and qualitatively assessing smoothness in transitions and sample diversity.</li>
  </ul>
  <p><strong>Implementation Notes:</strong> The experiments are implemented in PyTorch, utilizing modules such as <code>torch.cdist</code> for pairwise distance computations and standard techniques like WGAN gradient penalty or spectral normalization in the critic. Hyperparameters, including batch size and learning rates, remain consistent across comparable runs. Final models and plotted results are saved as PDF files for reproducibility.</p>
</section>

<section>
  <h2>Results</h2>
  <p>The experimental results are summarized in three categories, with corresponding figures provided below.</p>
  <p><strong>1) Geometry Preservation with Isometric Regularization</strong></p>
  <figure>
    <img src="images/recon_error_comparison.png" style="width:70%;height:auto" alt="Recon Error Comparison">
    <figcaption>Figure 1: Recon Error Comparison (recon_error_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.1.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.1">
    <figcaption>Figure 2: Stochastic Diversity at σ=0.1 (stochastic_diversity_sigma0.1.png)</figcaption>
  </figure>
  <figure>
    <img src="images/lambda_comparison.png" style="width:70%;height:auto" alt="Lambda Comparison">
    <figcaption>Figure 3: Lambda Comparison (lambda_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.5.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.5">
    <figcaption>Figure 4: Stochastic Diversity at σ=0.5 (stochastic_diversity_sigma0.5.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda1.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=1.0">
    <figcaption>Figure 5: Latent Interpolation with λ_iso=1.0 (latent_interpolation_lambda1.0.png)</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.0.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.0">
    <figcaption>Figure 6: Stochastic Diversity at σ=0.0 (stochastic_diversity_sigma0.0.png)</figcaption>
  </figure>
  <p><strong>2) Partial Noise Injection and Mode Coverage</strong></p>
  <figure>
    <img src="images/latent_interpolation_Iso.png" style="width:70%;height:auto" alt="Latent Interpolation in Iso-LWGAN">
    <figcaption>Figure 7: Latent Interpolation in Iso-LWGAN (latent_interpolation_Iso.png)</figcaption>
  </figure>
  <figure>
    <img src="images/sigma_loss_comparison.png" style="width:70%;height:auto" alt="Sigma Loss Comparison">
    <figcaption>Figure 8: Sigma Loss Comparison (sigma_loss_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/distance_preservation.png" style="width:70%;height:auto" alt="Distance Preservation">
    <figcaption>Figure 9: Distance Preservation (distance_preservation.png)</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.0.png" style="width:70%;height:auto" alt="Noise Injection σ=0.0">
    <figcaption>Figure 10: Noise Injection σ=0.0 (noise_injection_sigma0.0.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda0.5.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=0.5">
    <figcaption>Figure 11: Latent Interpolation with λ_iso=0.5 (latent_interpolation_lambda0.5.png)</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.1.png" style="width:70%;height:auto" alt="Noise Injection σ=0.1">
    <figcaption>Figure 12: Noise Injection σ=0.1 (noise_injection_sigma0.1.png)</figcaption>
  </figure>
  <p><strong>3) Overall Comparison on MNIST</strong></p>
  <figure>
    <img src="images/mnist_comparison.png" style="width:70%;height:auto" alt="MNIST Comparison">
    <figcaption>Figure 13: MNIST Comparison (mnist_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/diversity_comparison.png" style="width:70%;height:auto" alt="Diversity Comparison">
    <figcaption>Figure 14: Diversity Comparison (diversity_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_Base.png" style="width:70%;height:auto" alt="Latent Interpolation in Base LWGAN">
    <figcaption>Figure 15: Latent Interpolation in Base LWGAN (latent_interpolation_Base.png)</figcaption>
  </figure>
  <figure>
    <img src="images/avg_diff_comparison.png" style="width:70%;height:auto" alt="Average Distance Differences">
    <figcaption>Figure 16: Average Distance Differences (avg_diff_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda2.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=2.0">
    <figcaption>Figure 17: Latent Interpolation with λ_iso=2.0 (latent_interpolation_lambda2.0.png)</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.5.png" style="width:70%;height:auto" alt="Noise Injection σ=0.5">
    <figcaption>Figure 18: Noise Injection σ=0.5 (noise_injection_sigma0.5.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda0.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=0.0">
    <figcaption>Figure 19: Latent Interpolation with λ_iso=0.0 (latent_interpolation_lambda0.0.png)</figcaption>
  </figure>
  <p>Quantitatively, Iso-LWGAN often achieves improved mode coverage and smoother transitions, though occasionally at the cost of mild increases in reconstruction error. By tuning λ<sub>iso</sub> and σ_noise, practitioners can strike a balance between strict distance preservation, stable manifold coverage, and reconstruction objectives. The MNIST evaluation highlights these trade-offs by showing that even a modest isometric penalty and moderate noise injection yield more interpretable latent traversals and better coverage of digit classes.</p>
</section>

<section>
  <h2>Conclusions</h2>
  <p>We introduced Iso-LWGAN, which extends LWGAN by incorporating an isometric regularizer and selective noise injection in the generator. These enhancements address crucial issues of local geometric fidelity and mode coverage without sacrificing the underlying dimension-adaptive approach.</p>
  <p>Our experiments on synthetic manifolds and MNIST reveal that geometric distortions decrease substantially at higher isometric penalty levels, while partial noise injection mitigates mode collapse in multimodal data. Although isometric regularization can increase reconstruction loss, the principled alignment of latent and output geometries can yield smoother interpolations and more coherent transitions.</p>
  <p>In practice, tuning λ<sub>iso</sub> and σ_noise allows balancing geometry, variability, and reconstruction quality. Future work includes applying Iso-LWGAN to higher-resolution images and exploring advanced manifold constraints, potentially involving curvature or topological features, to further enhance the capacity of generative models to reflect the true underlying structure of complex data.</p>
</section>
</body>
</html>