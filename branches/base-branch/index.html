
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">Iso-LWGAN: A Geometry-Aware Approach to Adaptive Generative Modeling</h2>

<section>
  <h2>Abstract</h2>
  <p>We introduce Iso-LWGAN, an adaptive dimension-learning framework that builds on the Latent Wasserstein GAN (LWGAN) by integrating an isometric regularization term and partial stochasticity in the generator. Our goal is to address the challenge of learning data manifolds whose intrinsic dimensions are lower than those of the ambient space while also preserving local distances for enhanced interpretability.</p>
  <p>The isometric regularizer encourages the generator to maintain geometric fidelity between latent codes and output samples, whereas the partially stochastic generator captures the multimodality often present in real-world data. This combination tackles issues of latent mismatch and mode collapse reported in previous GAN-based methods.</p>
  <p>We quantitatively and qualitatively validate Iso-LWGAN on synthetic manifolds and real datasets such as MNIST, showing that our method preserves local geometry, detects manifold dimensions, and achieves improved coverage of data modes. Experiments reveal that aligning latent distances with generated sample distances leads to smoother interpolations, whereas introducing carefully controlled noise inside the generator helps mitigate mode collapse. Our results underscore Iso-LWGAN’s potential as a powerful and flexible tool for manifold-aware generative modeling.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Generative modeling seeks to learn underlying distributions from observed data, enabling the generation of new, high-quality samples that approximate real-world complexity. Classic latent-variable models such as Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) have demonstrated remarkable success in areas including image synthesis, text generation, and audio modeling. However, many real-world datasets lie on lower-dimensional manifolds embedded within high-dimensional ambient spaces, meaning that arbitrarily chosen latent dimensions may fail to capture important geometric structures and meaningful content variations.</p>
  <p>Recent work has emphasized the benefits of manifold-awareness for generative models. The Latent Wasserstein GAN (LWGAN) was proposed to adaptively estimate the manifold’s intrinsic dimension by learning a latent distribution with a diagonal covariance matrix whose rank corresponds to the data dimensionality. LWGAN combines concepts from Wasserstein Auto-Encoders (WAE) and Wasserstein GANs (WGAN) to yield consistent dimension estimates, but limitations persist. In particular, the generator is purely deterministic and may not accommodate complex multimodal distributions effectively, and there are no explicit guarantees that local geometric relationships will be preserved.</p>
  <p>Concurrently, isometric or distance-preserving techniques have gained traction in representation learning. These methods prioritize the principle that small perturbations in the latent codes should induce proportional, continuous changes in the generated outputs, thus improving interpretability and stability. Strictly deterministic mappings, however, can limit coverage of diverse data modes, reducing the model’s capacity to distribute probability mass over the often multimodal data manifold.</p>
  <p>In this paper, we propose Iso-LWGAN, an enhancement over LWGAN that addresses these gaps by incorporating an isometric regularizer and introducing partial stochasticity in generation. The key contributions are:</p>
  <ul>
    <li><strong>Distance-Preserving Penalty:</strong> Incorporates a penalty inspired by isometric representation learning to align local distances in the latent and output spaces, yielding smoother transitions and fewer geometric distortions.</li>
    <li><strong>Controlled Noise Injection:</strong> Injects controlled noise into the generator, striking a balance between deterministic and fully stochastic mappings, which helps avoid mode collapse while preserving stable training.</li>
    <li><strong>Dimension Consistency:</strong> Preserves the diagonal latent-covariance parameterization from LWGAN to ensure that the learned dimensionality matches the manifold dimension.</li>
    <li><strong>Comprehensive Experiments:</strong> Conducts experiments that isolate the effect of isometric regularization, analyze the benefits of partial noise, and compare Iso-LWGAN against the baseline LWGAN on MNIST.</li>
  </ul>
  <p>The remainder of this paper is organized as follows. We first position our approach in relation to prior manifold-based and geometry-conscious generative methods. Next, we summarize background concepts on dimension-adaptive latent variable models. We then detail the Iso-LWGAN architecture, focusing on how the isometric penalty and partial noise injection integrate with LWGAN’s dimension consistency. We follow with our experimental setup on synthetic data and real data (MNIST). Finally, we present comprehensive results showing local geometry improvements, better coverage, and structured latent spaces, and conclude with reflections on future directions for manifold-aware generative modeling.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>Generative adversarial networks have progressed substantially since their inception, largely focusing on broader distribution matching in Euclidean space. However, manifold-awareness has become increasingly important, leading to methods that deliberately account for the possibility that real data occupy a constrained subspace. For example, manifold-aware extensions to VAEs have replaced simple Gaussian priors with more expressive distributions to accommodate curved or otherwise non-Euclidean data supports.</p>
  <p>Dimension-consistent techniques specifically aim to match the latent space dimension to that of the underlying data manifold. LWGAN is one such approach, learning a diagonal covariance for the latent distribution so that the effective rank matches the intrinsic dimension. Meanwhile, WGAN variants rely on a Wasserstein metric for stable training, but they typically do not tackle dimension estimation or local distance preservation.</p>
  <p>Beyond dimension adaptivity, an emerging research direction explores isometric representation learning, wherein the goal is to preserve local distances. Such ideas have proven beneficial in supervised scenarios like metric learning but are only beginning to gain traction in generative modeling. Ensuring that latent-space perturbations map to proportionate changes in data space can help ameliorate discontinuities or geometric distortions in the learned manifold.</p>
  <p>Partial or selective noise injection has also been proposed to mitigate mode collapse. Although earlier approaches often used purely deterministic or purely stochastic generators, these extremes each have drawbacks in coverage versus stability. A “hybrid” methodology—feeding a modest noise vector alongside the latent code—can expand the distribution of outputs for each latent code without severely disrupting training. Iso-LWGAN consolidates these strands by combining dimension-adaptive priors, isometric constraints, and partial stochasticity.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Real-world data frequently reside on manifolds of lower dimension than the space in which they physically appear. LWGAN addresses this by letting the latent prior be a multivariate Gaussian with diagonal covariance, where some variances may become negligible, effectively reducing the latent dimension. Formally, if the data manifold X &isin; ℝ<sup>D</sup> has dimension m, then a rank-m diagonal matrix A captures variance in m directions while suppressing variance in the others. The learned parameters dictate which directions remain active.</p>
  <p>Adversarial training in LWGAN is based on minimizing the Wasserstein distance between real and generated data distributions. One typically enforces a 1-Lipschitz constraint on the critic (discriminator) using gradient penalty or a similar technique.</p>
  <p>Although dimension adaptation offers benefits for manifold alignment, local geometry could still be distorted because the WGAN objective does not explicitly penalize differences in local distances. Moreover, a purely deterministic generator can struggle to reproduce multimodal distributions, leading to partial collapse of modes. Integrating an isometric constraint and partial noise injection can address these problems:</p>
  <ul>
    <li><strong>Isometric Representation Learning:</strong> Incorporates a penalty on the absolute difference between pairwise distances in latent codes and in generated outputs, thereby preserving local geometry.</li>
    <li><strong>Partial Stochasticity:</strong> By concatenating a small noise vector ε with the latent code z, multiple potential outcomes from the same z are enabled, enhancing coverage and reducing collapse. The parameter σ governs the influence of noise on generation.</li>
  </ul>
</section>

<section>
  <h2>Method</h2>
  <p>Iso-LWGAN synthesizes three main ideas for manifold-aware generative modeling: dimension adaptation, isometric regularization, and partial stochasticity.</p>
  <p><strong>1) Adaptive Latent Dimensionality:</strong> We adopt the LWGAN notion of a diagonal covariance matrix A for the latent normal distribution. By learning which directions in ℝ<sup>d</sup> are truly active, the model approximates the intrinsic manifold dimension. The overall architecture includes an encoder Q: X → Z that maps data to latent codes and a generator G: Z → X that attempts to reconstruct data from latent codes.</p>
  <p><strong>2) Isometric Regularization:</strong> To promote distance preservation, we add a term that penalizes the mismatch between pairwise distances. Concretely, for latent codes z<sub>i</sub> and z<sub>j</sub> and generated points x<sub>i</sub> and x<sub>j</sub>, the loss can be described as:</p>
  <pre><code>L_iso = λ_iso * (1/N²) * Σᵢⱼ | ||zᵢ - zⱼ||₂ - ||xᵢ - xⱼ||₂ |</code></pre>
  <p>The hyperparameter λ_iso controls how strictly local distances in the latent space must match those in the generated space.</p>
  <p><strong>3) Partially Stochastic Generator:</strong> Instead of using a fully deterministic generator, we add a noise vector ε drawn from a low-dimensional normal distribution N(0, σ²I). The generator input is then provided as [z; ε]. A suitably chosen σ helps expand the range of generated outputs without undermining training stability. This approach allows a single latent code to map to multiple modes, especially when the real data exhibit distinct clusters.</p>
  <p><strong>Overall Training Objective:</strong> We retain the standard WGAN-based adversarial objective, possibly with reconstruction and consistency terms for encoding as in LWGAN. The total loss becomes:</p>
  <pre><code>L_total = L_Wasserstein + L_iso</code></pre>
  <p>Here, L_Wasserstein captures the adversarial terms and L_iso is scaled by λ_iso. This formulation balances stable manifold coverage, local geometry preservation, and dimension adaptivity in one unified framework.</p>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>We design three separate experiments to verify the contributions of the isometric regularizer, partial generator noise, and the combined Iso-LWGAN approach.</p>
  <p><strong>1) Geometry Preservation on Synthetic 2D Data:</strong></p>
  <ul>
    <li><strong>Dataset:</strong> A two-component Gaussian mixture in ℝ<sup>2</sup>.</li>
    <li><strong>Goal:</strong> To observe how varying λ_iso ∈ {0, 0.1, 0.5, 1.0, 2.0} influences distance alignment, measuring the average absolute difference between pairwise distances in the latent and generated spaces, tracking reconstruction error, and visualizing latent interpolations.</li>
    <li><strong>Architecture:</strong> A simple encoder and generator with two hidden layers, using Adam optimizer over a moderate number of epochs.</li>
  </ul>
  <p><strong>2) Partially Stochastic Generator for Multimodal Data:</strong></p>
  <ul>
    <li><strong>Dataset:</strong> A 2D multimodal dataset with multiple clusters.</li>
    <li><strong>Goal:</strong> To investigate the effect of partial noise injection by varying σ_noise ∈ {0, 0.1, 0.5} and evaluating its impact on mode coverage and the mitigation of mode collapse.</li>
    <li><strong>Metrics:</strong> Qualitative inspection of sample diversity and quantitative measures of training stability under different noise levels.</li>
  </ul>
  <p><strong>3) MNIST Comparison: Iso-LWGAN vs. Base LWGAN:</strong></p>
  <ul>
    <li><strong>Dataset:</strong> MNIST digits (28×28 images), noting that the underlying manifold dimension is likely much smaller than 784.</li>
    <li><strong>Goal:</strong> To directly compare a baseline LWGAN (without isometric term and noise injection) to Iso-LWGAN (with nonzero λ_iso and partial noise), using a shared feedforward encoder/generator architecture aside from noise injection.</li>
    <li><strong>Measurements:</strong> Tracking reconstruction and total losses, evaluating digit coverage, generating latent interpolations, and assessing the smoothness and diversity of generated samples.</li>
  </ul>
  <p><strong>Implementation Notes:</strong> The experiments are implemented in PyTorch, utilizing modules such as <code>torch.cdist</code> for pairwise distances and standard techniques like gradient penalty or spectral normalization for enforcing the 1-Lipschitz constraint in the critic. Hyperparameters remain consistent across comparable runs, and all code saves final models and plotted results as PDF files for reproducibility.</p>
</section>

<section>
  <h2>Results</h2>
  <p>The outcomes of our experiments are summarized in the following figures.</p>
  <p><strong>1) Geometry Preservation with Isometric Regularization</strong></p>
  <figure>
    <img src="images/recon_error_comparison.png" style="width:70%;height:auto" alt="Recon Error Comparison">
    <figcaption>Figure 1: Recon Error Comparison (recon_error_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.1.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.1">
    <figcaption>Figure 2: Stochastic Diversity at σ=0.1 (stochastic_diversity_sigma0.1.png)</figcaption>
  </figure>
  <figure>
    <img src="images/lambda_comparison.png" style="width:70%;height:auto" alt="Lambda Comparison">
    <figcaption>Figure 3: Lambda Comparison (lambda_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.5.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.5">
    <figcaption>Figure 4: Stochastic Diversity at σ=0.5 (stochastic_diversity_sigma0.5.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda1.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=1.0">
    <figcaption>Figure 5: Latent Interpolation with λ_iso=1.0 (latent_interpolation_lambda1.0.png)</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.0.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.0">
    <figcaption>Figure 6: Stochastic Diversity at σ=0.0 (stochastic_diversity_sigma0.0.png)</figcaption>
  </figure>

  <p><strong>2) Partial Noise Injection and Mode Coverage</strong></p>
  <figure>
    <img src="images/latent_interpolation_Iso.png" style="width:70%;height:auto" alt="Latent Interpolation in Iso-LWGAN">
    <figcaption>Figure 7: Latent Interpolation in Iso-LWGAN (latent_interpolation_Iso.png)</figcaption>
  </figure>
  <figure>
    <img src="images/sigma_loss_comparison.png" style="width:70%;height:auto" alt="Sigma Loss Comparison">
    <figcaption>Figure 8: Sigma Loss Comparison (sigma_loss_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/distance_preservation.png" style="width:70%;height:auto" alt="Distance Preservation">
    <figcaption>Figure 9: Distance Preservation (distance_preservation.png)</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.0.png" style="width:70%;height:auto" alt="Noise Injection σ=0.0">
    <figcaption>Figure 10: Noise Injection σ=0.0 (noise_injection_sigma0.0.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda0.5.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=0.5">
    <figcaption>Figure 11: Latent Interpolation with λ_iso=0.5 (latent_interpolation_lambda0.5.png)</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.1.png" style="width:70%;height:auto" alt="Noise Injection σ=0.1">
    <figcaption>Figure 12: Noise Injection σ=0.1 (noise_injection_sigma0.1.png)</figcaption>
  </figure>

  <p><strong>3) Overall Comparison on MNIST</strong></p>
  <figure>
    <img src="images/mnist_comparison.png" style="width:70%;height:auto" alt="MNIST Comparison">
    <figcaption>Figure 13: MNIST Comparison (mnist_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/diversity_comparison.png" style="width:70%;height:auto" alt="Diversity Comparison">
    <figcaption>Figure 14: Diversity Comparison (diversity_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_Base.png" style="width:70%;height:auto" alt="Latent Interpolation in Base LWGAN">
    <figcaption>Figure 15: Latent Interpolation in Base LWGAN (latent_interpolation_Base.png)</figcaption>
  </figure>
  <figure>
    <img src="images/avg_diff_comparison.png" style="width:70%;height:auto" alt="Average Distance Differences">
    <figcaption>Figure 16: Average Distance Differences (avg_diff_comparison.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda2.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=2.0">
    <figcaption>Figure 17: Latent Interpolation with λ_iso=2.0 (latent_interpolation_lambda2.0.png)</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.5.png" style="width:70%;height:auto" alt="Noise Injection σ=0.5">
    <figcaption>Figure 18: Noise Injection σ=0.5 (noise_injection_sigma0.5.png)</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda0.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=0.0">
    <figcaption>Figure 19: Latent Interpolation with λ_iso=0.0 (latent_interpolation_lambda0.0.png)</figcaption>
  </figure>
</section>

<section>
  <h2>Conclusions</h2>
  <p>We introduced Iso-LWGAN, which extends LWGAN by incorporating an isometric regularizer and selective noise injection in the generator. These enhancements address crucial issues of local geometric fidelity and mode coverage without sacrificing the underlying dimension-adaptive approach.</p>
  <p>Our experiments on synthetic manifolds and MNIST reveal that geometric distortions decrease substantially at higher isometric penalty levels, while partial noise injection mitigates mode collapse in multimodal data.</p>
  <p>Although isometric regularization can increase reconstruction loss, the principled alignment of latent and output geometries yields smoother interpolations and more coherent transitions. In practice, tuning λ_iso and σ_noise enables a balance between geometry preservation, variability, and reconstruction quality. Future work includes applying Iso-LWGAN to higher-resolution images and exploring advanced manifold constraints, potentially involving curvature or topological features, to further enhance generative modeling of complex data structures.</p>
</section>
</body>
</html>