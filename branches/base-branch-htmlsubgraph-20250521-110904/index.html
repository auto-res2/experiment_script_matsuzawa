
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">Iso-LWGAN: A Geometry-Aware Approach to Adaptive Generative Modeling</h2>

<section>
  <h2>Abstract</h2>
  <p>We introduce Iso-LWGAN, an adaptive dimension-learning framework that builds on the Latent Wasserstein GAN (LWGAN) by integrating an isometric regularization term and partial stochasticity in the generator. Our goal is to address the challenge of learning data manifolds whose intrinsic dimensions are lower than those of the ambient space while also preserving local distances for enhanced interpretability.</p>
  <p>The isometric regularizer encourages the generator to maintain geometric fidelity between latent codes and output samples, whereas the partially stochastic generator captures the multimodality often present in real-world data. This combination tackles issues of latent mismatch and mode collapse reported in previous GAN-based methods.</p>
  <p>We quantitatively and qualitatively validate Iso-LWGAN on synthetic manifolds and real datasets such as MNIST, showing that our method preserves local geometry, detects manifold dimensions, and achieves improved coverage of data modes. Experiments reveal that aligning latent distances with generated sample distances leads to smoother interpolations, whereas introducing carefully controlled noise inside the generator helps mitigate mode collapse. Our results underscore Iso-LWGAN’s potential as a powerful and flexible tool for manifold-aware generative modeling.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Generative modeling seeks to learn underlying distributions from observed data, enabling the generation of new, high-quality samples that approximate real-world complexity. Classic latent-variable models such as Variational Auto-Encoders (VAEs) and Generative Adversarial Networks (GANs) have demonstrated remarkable success in areas including image synthesis, text generation, and audio modeling. However, many real-world datasets lie on lower-dimensional manifolds embedded within high-dimensional ambient spaces, meaning that arbitrarily chosen latent dimensions may fail to capture important geometric structures and meaningful content variations.</p>
  <p>Recent work has emphasized the benefits of manifold-awareness for generative models. The Latent Wasserstein GAN (LWGAN) was proposed to adaptively estimate the manifold’s intrinsic dimension by learning a latent distribution with a diagonal covariance matrix whose rank corresponds to the data dimensionality. LWGAN combines concepts from Wasserstein Auto-Encoders (WAE) and Wasserstein GANs (WGAN) to yield consistent dimension estimates, but limitations persist. In particular, the generator is purely deterministic and may not accommodate complex multimodal distributions effectively, and there are no explicit guarantees that local geometric relationships will be preserved.</p>
  <p>Concurrently, isometric or distance-preserving techniques have gained traction in representation learning. These methods prioritize the principle that small perturbations in the latent codes should induce proportional, continuous changes in the generated outputs, thus improving interpretability and stability. Strictly deterministic mappings, however, can limit coverage of diverse data modes, reducing the model’s capacity to distribute probability mass over the often multimodal data manifold.</p>
  <p>In this paper, we propose Iso-LWGAN, an enhancement over LWGAN that addresses these gaps by incorporating an isometric regularizer and introducing partial stochasticity in generation. The key contributions are:</p>
  <ul>
    <li><strong>Distance-Preserving Penalty:</strong> We incorporate a penalty that aligns local distances in the latent and output spaces, yielding smoother transitions and fewer geometric distortions.</li>
    <li><strong>Partial Noise Injection:</strong> We inject controlled noise into the generator to strike a balance between deterministic and stochastic mappings, helping to avoid mode collapse while preserving stable training.</li>
    <li><strong>Dimension Consistency:</strong> We preserve the diagonal latent-covariance parameterization from LWGAN, ensuring that the learned dimensionality closely approximates the underlying manifold dimension.</li>
    <li><strong>Comprehensive Evaluation:</strong> We conduct experiments that isolate the effects of isometric regularization and partial noise, comparing Iso-LWGAN against a baseline LWGAN on MNIST.</li>
  </ul>
  <p>The remainder of this paper is organized as follows. We first position our approach in relation to prior manifold-based and geometry-conscious generative methods. Next, we summarize background concepts on dimension-adaptive latent variable models. We then detail the Iso-LWGAN architecture, focusing on how the isometric penalty and partial noise injection integrate with LWGAN’s dimension consistency. We follow with our experimental setup on synthetic data and real data (MNIST). Finally, we present comprehensive results showing local geometry improvements, better coverage, and structured latent spaces, and conclude with reflections on future directions for manifold-aware generative modeling.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>Generative adversarial networks have progressed substantially since their inception, largely focusing on broader distribution matching in Euclidean space. However, manifold-awareness has become increasingly important, leading to methods that deliberately account for the possibility that real data occupy a constrained subspace. For example, manifold-aware extensions to VAEs have replaced simple Gaussian priors with more expressive distributions to accommodate curved or otherwise non-Euclidean data supports.</p>
  <p>Dimension-consistent techniques specifically aim to match the latent space dimension to that of the underlying data manifold. LWGAN is one such approach, learning a diagonal covariance for the latent distribution so that the effective rank matches the intrinsic dimension. Meanwhile, WGAN variants rely on a Wasserstein metric for stable training, but they typically do not tackle dimension estimation or local distance preservation.</p>
  <p>Beyond dimension adaptivity, an emerging research direction explores isometric representation learning, wherein the goal is to preserve local distances. Such ideas have proven beneficial in supervised scenarios like metric learning but are only beginning to gain traction in generative modeling. Ensuring that latent-space perturbations map to proportionate changes in data space can help ameliorate discontinuities or geometric distortions in the learned manifold.</p>
  <p>Partial or selective noise injection has also been proposed to mitigate mode collapse. Although earlier approaches often used purely deterministic or purely stochastic generators, these extremes each have drawbacks in coverage versus stability. A “hybrid” methodology—feeding a modest noise vector alongside the latent code—can expand the distribution of outputs for each latent code without severely disrupting training. Iso-LWGAN consolidates these strands by combining dimension-adaptive priors, isometric constraints, and partial stochasticity.</p>
</section>

<section>
  <h2>Background</h2>
  <p>Real-world data frequently reside on manifolds of lower dimension than the space in which they physically appear. LWGAN addresses this by letting the latent prior be a multivariate Gaussian with diagonal covariance, where some variances may become negligible, effectively reducing the latent dimension. Formally, if the data manifold X ⊂ ℝ<sup>D</sup> has dimension m, then a rank-m diagonal matrix captures variance in m directions while suppressing variance in the others. The learned parameters dictate which directions remain active.</p>
  <p>Adversarial training in LWGAN is based on minimizing the Wasserstein distance between real and generated data distributions. One typically enforces a 1-Lipschitz constraint on the critic (discriminator) using gradient penalty or a similar technique.</p>
  <p>Although dimension adaptation offers benefits for manifold alignment, local geometry could still be distorted because the WGAN objective does not explicitly penalize differences in local distances. Moreover, a purely deterministic generator can struggle to reproduce multimodal distributions, leading to partial collapse of modes. Integrating an isometric constraint and partial noise injection can address these problems:</p>
  <ul>
    <li><strong>Isometric Representation Learning:</strong> A penalty on the absolute difference between pairwise distances in latent codes and generated outputs is incorporated to preserve local geometry.</li>
    <li><strong>Partial Stochasticity:</strong> By concatenating a small noise vector with the latent code, the model permits multiple outcomes from the same latent code, enhancing output diversity while maintaining stability.</li>
  </ul>
</section>

<section>
  <h2>Method</h2>
  <p>Iso-LWGAN synthesizes three main ideas for manifold-aware generative modeling:</p>
  <ul>
    <li><strong>Adaptive Latent Dimensionality:</strong> We adopt the LWGAN notion of a diagonal covariance matrix for the latent normal distribution. By learning which directions in ℝ<sup>d</sup> are truly active, the model approximates the intrinsic manifold dimension. The architecture includes an encoder mapping data to latent codes and a generator that reconstructs data from these codes.</li>
    <li><strong>Isometric Regularization:</strong> A term is added to penalize mismatches in pairwise distances between latent codes and generated data. This encourages the preservation of local geometric relationships.</li>
    <li><strong>Partially Stochastic Generator:</strong> Instead of a purely deterministic generator, a noise vector drawn from a low-dimensional normal distribution is concatenated with the latent code. This selective noise injection allows a single latent code to map to multiple modes, improving output diversity.</li>
  </ul>
  <p>Overall Training Objective</p>
  <pre><code>L_total = L_Wasserstein + L_iso

# where L_Wasserstein captures the standard adversarial loss
# and L_iso is scaled by the hyperparameter λ_iso to enforce local distance preservation</code></pre>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>We use three separate experiments to verify the contributions of the isometric regularizer, partial generator noise, and the combined Iso-LWGAN approach.</p>
  <ul>
    <li><strong>Geometry Preservation on Synthetic 2D Data:</strong> 
      <p>- <strong>Dataset:</strong> A two-component Gaussian mixture in ℝ<sup>2</sup>.</p>
      <p>- <strong>Goal:</strong> To observe how varying λ_iso influences distance alignment, measuring average absolute differences between pairwise distances and reconstruction errors, along with visualizing latent interpolations.</p>
      <p>- <strong>Architecture:</strong> A simple encoder and generator with two hidden layers using the Adam optimizer over a moderate number of epochs.</p>
    </li>
    <li><strong>Partially Stochastic Generator for Multimodal Data:</strong> 
      <p>- <strong>Dataset:</strong> A 2D multimodal dataset with multiple clusters.</p>
      <p>- <strong>Goal:</strong> Investigate partial noise injection by varying σ_noise and examining improvements in mode coverage and reduction of mode collapse.</p>
      <p>- <strong>Metrics:</strong> Qualitative sample diversity and training stability under different noise magnitudes.</p>
    </li>
    <li><strong>MNIST Comparison: Iso-LWGAN vs. Base LWGAN:</strong> 
      <p>- <strong>Dataset:</strong> MNIST digits, where the actual manifold dimension is much smaller than the image dimension.</p>
      <p>- <strong>Goal:</strong> Directly compare a baseline LWGAN (without isometric term or noise injection) to Iso-LWGAN using similar architectures apart from noise injection.</p>
      <p>- <strong>Measurements:</strong> Reconstruction and total losses tracking, visual and quantitative assessments of digit mode coverage, and latent interpolation smoothness.</p>
    </li>
  </ul>
  <p>Implementation is done using PyTorch, leveraging modules like <code>torch.cdist</code> for pairwise distances and standard techniques for enforcing the 1-Lipschitz constraint in the critic. Consistent hyperparameters are maintained for fairness in comparisons, and final models along with plotted results are saved as PNG files for reproducibility.</p>
</section>

<section>
  <h2>Results</h2>
  <p>The outcomes of our experiments are summarized below. Each figure is provided as a static image for visual reference.</p>
  <figure>
    <img src="images/recon_error_comparison.png" style="width:70%;height:auto" alt="Recon Error Comparison">
    <figcaption>Figure 1: Recon Error Comparison shows the evolution of reconstruction error at different λ_iso values for our two-component Gaussian mixture.</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.1.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.1">
    <figcaption>Figure 2: Stochastic Diversity at σ=0.1 confirms that local geometry remains coherent under moderate noise.</figcaption>
  </figure>
  <figure>
    <img src="images/lambda_comparison.png" style="width:70%;height:auto" alt="Lambda Comparison">
    <figcaption>Figure 3: Lambda Comparison depicts how the average distance differences between latent codes and generated outputs drop as λ_iso increases.</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.5.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.5">
    <figcaption>Figure 4: Stochastic Diversity at σ=0.5 includes samples highlighting geometry retention under higher noise.</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda1.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=1.0">
    <figcaption>Figure 5: Latent Interpolation with λ_iso=1.0 visualizes smoother transitions between latent codes at a nonzero isometric penalty.</figcaption>
  </figure>
  <figure>
    <img src="images/stochastic_diversity_sigma0.0.png" style="width:70%;height:auto" alt="Stochastic Diversity at σ=0.0">
    <figcaption>Figure 6: Stochastic Diversity at σ=0.0 demonstrates minimal variability in output when the generator is essentially deterministic.</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_Iso.png" style="width:70%;height:auto" alt="Latent Interpolation in Iso-LWGAN">
    <figcaption>Figure 7: Latent Interpolation in Iso-LWGAN illustrates smooth transitions in a geometry-aware, partially stochastic setting.</figcaption>
  </figure>
  <figure>
    <img src="images/sigma_loss_comparison.png" style="width:70%;height:auto" alt="Sigma Loss Comparison">
    <figcaption>Figure 8: Sigma Loss Comparison summarizes training stability and reconstruction performance at different σ_noise values.</figcaption>
  </figure>
  <figure>
    <img src="images/distance_preservation.png" style="width:70%;height:auto" alt="Distance Preservation">
    <figcaption>Figure 9: Distance Preservation shows aggregated statistics of geometry retention for varying λ_iso and σ_noise.</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.0.png" style="width:70%;height:auto" alt="Noise Injection σ=0.0">
    <figcaption>Figure 10: Noise Injection σ=0.0 depicts samples with no added noise, confirming near-identical outputs for a given latent code.</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda0.5.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=0.5">
    <figcaption>Figure 11: Latent Interpolation with λ_iso=0.5 demonstrates that moderate isometric penalties still preserve smooth transitions.</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.1.png" style="width:70%;height:auto" alt="Noise Injection σ=0.1">
    <figcaption>Figure 12: Noise Injection σ=0.1 highlights the controlled variability in outputs from the same latent code.</figcaption>
  </figure>
  <figure>
    <img src="images/mnist_comparison.png" style="width:70%;height:auto" alt="MNIST Comparison">
    <figcaption>Figure 13: MNIST Comparison presents a side-by-side visual of generated images from Base LWGAN and Iso-LWGAN.</figcaption>
  </figure>
  <figure>
    <img src="images/diversity_comparison.png" style="width:70%;height:auto" alt="Diversity Comparison">
    <figcaption>Figure 14: Diversity Comparison summarizes the coverage of digit classes, showing that Base LWGAN occasionally overlooks certain modes.</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_Base.png" style="width:70%;height:auto" alt="Latent Interpolation in Base LWGAN">
    <figcaption>Figure 15: Latent Interpolation in Base LWGAN illustrates less smooth or consistent transitions compared to Iso-LWGAN.</figcaption>
  </figure>
  <figure>
    <img src="images/avg_diff_comparison.png" style="width:70%;height:auto" alt="Average Distance Differences">
    <figcaption>Figure 16: Average Distance Differences summarizes geometry metrics, revealing improved local distance preservation with Iso-LWGAN.</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda2.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=2.0">
    <figcaption>Figure 17: Latent Interpolation with λ_iso=2.0 presents an extreme isometric setting with strongly enforced distance preservation.</figcaption>
  </figure>
  <figure>
    <img src="images/noise_injection_sigma0.5.png" style="width:70%;height:auto" alt="Noise Injection σ=0.5">
    <figcaption>Figure 18: Noise Injection σ=0.5 displays increased diversity in outputs at higher noise levels, with a careful trade-off against stability.</figcaption>
  </figure>
  <figure>
    <img src="images/latent_interpolation_lambda0.0.png" style="width:70%;height:auto" alt="Latent Interpolation with λ_iso=0.0">
    <figcaption>Figure 19: Latent Interpolation with λ_iso=0.0 shows the impact of absent local distance regularization, leading to potential distortions in transitions.</figcaption>
  </figure>
</section>

<section>
  <h2>Conclusions</h2>
  <p>We introduced Iso-LWGAN, an extension of LWGAN that incorporates an isometric regularizer and selective noise injection in the generator. These enhancements address crucial issues of local geometric fidelity and coverage while retaining a dimension-adaptive approach.</p>
  <p>Our experiments on synthetic manifolds and MNIST reveal that higher isometric penalty levels substantially decrease geometric distortions, and that partial noise injection effectively mitigates mode collapse in multimodal data. Although isometric regularization may increase reconstruction loss slightly, the improved alignment between latent and output geometries yields smoother interpolations and more coherent transitions.</p>
  <p>In practice, careful tuning of λ_iso and σ_noise allows one to balance geometric accuracy, variability, and reconstruction quality. Future work could explore the application of Iso-LWGAN to higher-resolution images and incorporate advanced manifold constraints, such as curvature or topological features, to further enhance generative modeling of complex data.</p>
</section>
</body>
</html>