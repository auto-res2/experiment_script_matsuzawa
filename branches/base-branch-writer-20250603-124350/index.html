
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">Iso-LWGAN: Enhancing Adaptive Latent Learning with Isometric Regularization and Partial Stochasticity</h2>

<section>
  <h2>Abstract</h2>
  <p>This paper introduces Iso-LWGAN, a novel generative framework that extends the latent Wasserstein GAN (LWGAN) by integrating an isometric regularization term and a partially stochastic generator. The proposed method addresses latent mismatch and mode collapse issues inherent in standard LWGAN by enforcing local geometric fidelity in the latent space and by introducing controlled noise to capture multimodal data distributions.</p>
  <p>Iso-LWGAN learns a latent normal distribution with a diagonal covariance matrix whose rank reflects the intrinsic dimension of the data manifold. An isometric penalty minimizes the discrepancy between pairwise distances in the latent and generated spaces, and a noise injection step in the generator enables richer output diversity without destabilizing training. Comprehensive experiments on synthetic datasets and the MNIST benchmark demonstrate that the isometric regularizer produces smoother latent space interpolations, while the partially stochastic generator increases variability in generated outputs.</p>
  <p>Quantitative and qualitative analyses, including ablation studies and detailed loss and convergence plots, confirm the effectiveness of Iso-LWGAN in improving reconstruction, latent structure, and generative fidelity relative to the base LWGAN.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Recent advancements in generative modeling underscore the importance of learning latent representations that capture the intrinsic structure of data while preserving its geometric properties. Although successful frameworks such as GANs and VAEs have achieved impressive results, they face challenges when the data lies on a lower-dimensional manifold embedded within a high-dimensional ambient space.</p>
  <p>The latent Wasserstein GAN (LWGAN) was developed to adaptively learn such latent representations by modeling a latent normal distribution with a learned diagonal covariance matrix. However, LWGAN is subject to latent mismatch and struggles with the rigidity of a purely deterministic generator. In response, Iso-LWGAN is proposed to bridge these gaps by integrating isometric regularization and partial generator stochasticity.</p>
  <p>The isometric loss forces local distances in the latent space to align with those in the generated data, ensuring that small latent perturbations result in smooth, interpretable transitions. Additionally, incorporating controlled noise into the generator mitigates mode collapse and enhances the model’s ability to capture multimodal distributions.</p>
  <p>The contributions of this work can be summarized as follows:</p>
  <ul>
    <li><strong>Adaptive Latent Learning Scheme:</strong> An adaptive latent learning scheme where the effective rank of the learned diagonal covariance matrix reflects the true intrinsic dimension of the data manifold.</li>
    <li><strong>Isometric Regularizer:</strong> A novel isometric regularizer that penalizes absolute differences between pairwise distances in the latent and generated spaces to preserve local geometry.</li>
    <li><strong>Partially Stochastic Generator:</strong> A modification of the generator architecture that introduces a noise injection step, transforming a deterministic mapping into a partially stochastic process that yields diverse outputs.</li>
    <li><strong>Experimental Validation:</strong> Extensive experimental validation on synthetic datasets and the MNIST benchmark, demonstrating improvements in reconstruction accuracy, latent space smoothness, and generative fidelity.</li>
  </ul>
  <p>The paper is organized as follows. We first review related work, outline the necessary background on latent variable models and isometric learning, and then detail the proposed modifications to the LWGAN framework. Subsequently, we describe the experimental protocols and present the results, including detailed loss curves and convergence analyses evidenced by figures. Finally, conclusions and future research directions are discussed.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>The extensive literature on generative adversarial networks (GANs) has seen various efforts to improve latent representation learning and mitigate issues such as mode collapse. Techniques such as the Wasserstein GAN (WGAN) utilize the 1-Wasserstein distance along with gradient penalty or spectral normalization to enforce Lipschitz continuity in the critic, leading to improved stability. Similarly, the Wasserstein Auto-Encoder (WAE) combines reconstruction loss with distribution matching to capture underlying data manifolds.</p>
  <p>The base LWGAN framework itself fuses ideas from WGAN and WAE by learning a latent space whose dimension is automatically adapted via a diagonal covariance matrix. However, this method exhibits limitations regarding latent mismatch and the inability to capture output diversity due to its deterministic generator. Recent advances in isometric representation learning focus on preserving local geometric structure, an idea that has been successfully applied in manifold learning and dimensionality reduction.</p>
  <p>In parallel, strategies to inject noise into GAN generators have been explored to enhance diversity while balancing training stability. Iso-LWGAN distinguishes itself by integrating an isometric loss that directly penalizes discrepancies between pairwise distances in latent and generated spaces, and by adopting a partially stochastic generator that introduces controlled random perturbations to the latent code. This approach contrasts with alternatives that rely on fully stochastic generators or solely on gradient penalties and highlights the benefit of fusing adaptive latent dimension estimation with explicit geometric regularization and structured noise injection.</p>
</section>

<section>
  <h2>Background</h2>
  <p>A central problem in generative modeling is capturing the low-dimensional manifold on which most real-world data reside. Traditional methods, if not properly constrained, attempt to fill the full high-dimensional ambient space, often resulting in poor sample quality. The LWGAN framework addresses this by learning a latent normal distribution characterized by a diagonal covariance matrix, with the effective rank of this matrix matching the intrinsic dimensionality of the data.</p>
  <p>The encoder network Q maps data samples X to latent codes Z, while the generator G transforms these codes back to the data space. The use of the 1-Wasserstein distance, combined with a 1-Lipschitz critic, enforces population-level similarity between synthesized and real data distributions.</p>
  <p>Building on these ideas, isometric representation learning introduces a loss term that preserves local geometry by minimizing the absolute difference between the Euclidean distances in latent space and those in the output space. Formally, for latent codes z1 and z2 and corresponding generated outputs x1 and x2, the model minimizes the difference between ||z1 - z2|| and ||x1 - x2||. In addition, transition from a fully deterministic generator to one that incorporates a noise vector improves the model’s ability to capture multimodal distributions. This hybrid formulation, combining adaptive latent learning, isometric regularization, and partial stochasticity, forms the foundation of the Iso-LWGAN framework.</p>
</section>

<section>
  <h2>Method</h2>
  <p>Iso-LWGAN builds on the base LWGAN model by introducing three key modifications: adaptive latent dimensionality, isometric regularization, and a partially stochastic generator. First, the model employs an encoder network Q to map input data X to latent codes Z while learning a diagonal covariance matrix A whose effective rank aligns with the intrinsic data dimension. The generator G then maps these latent codes back to the data space, aiming to minimize an L2 reconstruction loss.</p>
  <p>Second, the isometric regularizer is incorporated to ensure that local geometric relationships are maintained. This is achieved by computing pairwise Euclidean distances in both latent space (using <code>torch.cdist</code>) and the generated output space, and then penalizing the average absolute difference between these distances, weighted by a hyperparameter λ_iso.</p>
  <p>Third, the generator is modified by adding a controlled noise injection step. Specifically, a noise vector ε sampled from a normal distribution (with standard deviation σ_noise) is concatenated with the latent code, resulting in a partially stochastic generator. The overall training objective is a composite loss consisting of the L2 reconstruction error, the Wasserstein adversarial loss (with gradient penalty or spectral normalization), and the isometric loss. Training follows a primal-dual iterative scheme where batches of data are processed to update the encoder-generator pair and the critic network alternately. This design effectively preserves the latent geometry, adapts the latent dimensionality to the data manifold, and increases output diversity without sacrificing training stability.</p>
  <pre><code># Pseudocode for the training loop
for epoch in range(num_epochs):
    for batch in data_loader:
        # Update encoder and generator
        latent_codes = Q(batch)
        reconstructed = G(latent_codes + noise(batch))
        loss_rec = L2_loss(batch, reconstructed)
        loss_iso = isometric_loss(latent_codes, reconstructed)
        loss_adv = wasserstein_loss(reconstructed, real_data)
        total_loss = loss_rec + loss_iso + loss_adv
        total_loss.backward()
        optimizer.step()

        # Update critic
        update_critic()
</code></pre>
</section>

<section>
  <h2>Experimental Setup</h2>
  <p>The effectiveness of Iso-LWGAN was evaluated through three distinct experiments implemented in Python using PyTorch.</p>
  <p>In Experiment 1, the impact of the isometric regularization term was examined on a synthetic 2D dataset generated by a mixture of two Gaussians. Various values of the hyperparameter λ_iso (0.0, 0.5, 1.0, and 2.0) were tested, and the average absolute difference between pairwise distances in the latent and generated spaces was measured. Latent space interpolations were visualized and saved as PDF plots to assess smoothness.</p>
  <p>Experiment 2 focused on evaluating the partially stochastic generator. A multimodal synthetic dataset with three clusters was used, and the generator was modified to accept both the latent code and a noise vector. Experiments were run for different noise levels (σ_noise = 0.0, 0.1, and 0.5) to quantify diversity in generated outputs through reconstruction losses and variability measurements.</p>
  <p>Finally, Experiment 3 involved a comparative study on the MNIST dataset, where both a base LWGAN model and the proposed Iso-LWGAN model were trained under similar conditions. Custom encoder and generator architectures were used, and training parameters (such as batch size, number of epochs, and learning rate) were kept consistent for a fair comparison. Detailed logging was performed using TensorBoard, and outputs including latent interpolation grids were saved as PDF files. Additional ablation studies were performed to isolate the contributions of the isometric regularizer and the noise injection component. Several figures (training_loss_base_pair1.pdf, convergence_solver_pair1.pdf, convergence_sequential_parallel_pair1.pdf, training_loss_taa_pair1.pdf) were also generated to illustrate training dynamics and convergence behavior across different configurations.</p>
</section>

<section>
  <h2>Results</h2>
  <p>The experimental results validate the benefits of Iso-LWGAN over the base LWGAN. In Experiment 1, introducing the isometric regularization term significantly reduced the average pairwise distance difference between the latent and generated spaces from approximately 1.20 (for λ_iso = 0.0) to values as low as 0.02–0.03 when λ_iso was set to 0.5, 1.0, and 2.0. Latent space interpolation visualizations (Figure 1) reveal smooth and continuous transitions in the generated outputs, reflecting well-preserved local geometry.</p>

  <figure>
    <img src="images/figure1.png" style="width:70%;height:auto" alt="Figure 1">
    <figcaption>Figure 1: Latent space interpolation visualizations demonstrating smooth and continuous transitions.</figcaption>
  </figure>

  <p>Experiment 2 demonstrated that a fully deterministic generator (σ_noise = 0.0) produced nearly identical outputs from a fixed latent code, whereas moderate noise levels (σ_noise = 0.1 and 0.5) introduced measurable diversity without destabilizing training. Figure 2 provides visualizations of the generated output distributions under different noise conditions, highlighting the benefits of controlled stochasticity.</p>

  <figure>
    <img src="images/figure2.png" style="width:70%;height:auto" alt="Figure 2">
    <figcaption>Figure 2: Generated output distributions under varying noise conditions.</figcaption>
  </figure>

  <p>In Experiment 3, a side-by-side comparison on MNIST indicated that while the base LWGAN achieved a slightly lower reconstruction error (approximately 5.62) compared to Iso-LWGAN (approximately 5.76) during a brief training phase, the Iso-LWGAN model produced noticeably smoother latent space interpolations and a more structurally coherent latent representation. This is illustrated by the latent interpolation grids in Figures 3 (base LWGAN) and 4 (Iso-LWGAN).</p>

  <figure>
    <img src="images/figure3.png" style="width:70%;height:auto" alt="Figure 3">
    <figcaption>Figure 3: Latent interpolation grid for base LWGAN.</figcaption>
  </figure>

  <figure>
    <img src="images/figure4.png" style="width:70%;height:auto" alt="Figure 4">
    <figcaption>Figure 4: Latent interpolation grid for Iso-LWGAN.</figcaption>
  </figure>

  <p>Additionally, the training dynamics and convergence behavior were further documented in the provided figures, which display detailed loss curves and solver performance comparisons.</p>

  <figure class="img-pair">
    <div class="pair">
      <img src="images/training_loss_base_pair1.png" style="width:48%;height:auto" alt="Training Loss Base Pair">
      <img src="images/convergence_solver_pair1.png" style="width:48%;height:auto" alt="Convergence Solver Pair">
    </div>
    <figcaption>Figure 5: Training loss (left) and convergence solver performance (right) comparisons.</figcaption>
  </figure>

  <figure class="img-pair">
    <div class="pair">
      <img src="images/convergence_sequential_parallel_pair1.png" style="width:48%;height:auto" alt="Convergence Sequential Parallel Pair">
      <img src="images/training_loss_taa_pair1.png" style="width:48%;height:auto" alt="Training Loss TAA Pair">
    </div>
    <figcaption>Figure 6: Convergence behavior (left) and training loss dynamics (right) for different configurations.</figcaption>
  </figure>
</section>

<section>
  <h2>Conclusions</h2>
  <p>In summary, this work presents Iso-LWGAN, a novel enhancement to the traditional latent Wasserstein GAN that incorporates both an isometric regularizer and a partially stochastic generator. The integration of a distance-preserving loss ensures that local geometric properties of the data manifold are retained in the latent space, while the introduction of controlled noise injection facilitates diverse generation and mitigates mode collapse.</p>
  <p>Experimental results on synthetic datasets and the MNIST benchmark demonstrate that Iso-LWGAN produces smoother latent space interpolations and robust generative performance relative to the base LWGAN, even if a minor increase in reconstruction error is observed in short training periods. The analysis, supported by quantitative metrics and detailed figures (including Figures 1 through 4 and additional training and convergence plots), confirms that the proposed approach effectively improves latent structure and output diversity.</p>
  <p>Future research directions include exploring fully stochastic generator variants, integrating Iso-LWGAN with high-resolution models such as BigGAN, and applying the framework to structural estimation in economic models. Overall, Iso-LWGAN represents a significant step toward more robust and interpretable generative adversarial networks by effectively merging adaptive latent dimension estimation with geometric preservation and enhanced output diversity.</p>
</section>
</body>
</html>